{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary libraries and specify that graphs should be plotted inline\n",
    "%matplotlib inline \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "# import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this short material, we apply gradient descent algorithm to linear models. Note that when applying gradient descent, it is important to make sure features are on a similar scale (otherwise, the algorithm can take a long time or even fail to converge). For now, we create a hypothetical dataset to guarantee this point.\n",
    "\n",
    "Consider a dataset with 1000 observations and 500 variables. All the X values are random draws from standard normal distribution. The relationship between Y and X is: $Y = 3 + \\theta X + e. $\n",
    "\n",
    "The cell below generates the hypothetical dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 500)\n"
     ]
    }
   ],
   "source": [
    "## Create a hypothetical dataset with 1000 rows, 500 variables.\n",
    "\n",
    "n_samples, n_features = 1000, 500  # define dimensions\n",
    "np.random.seed(0)                  # set seed for reproduction\n",
    "\n",
    "\n",
    "X = np.random.randn(n_samples, n_features) # a 1000 by 500 matrix, each entry is a random draw from standard normal distribution\n",
    "\n",
    "theta = np.random.rand(n_features)  # 500 features, so we need 500 parameters\n",
    "\n",
    "y = 3 + X.dot(theta) + np.random.randn(n_samples)  # define a hypothetical relationship\n",
    "\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We introduced three gradient descent variations: (1) batch gradient descent, (2) stochastic gradient descent, and (3) mini-batch gradient descent. Relying on sklearn package, we can only implement the second variation, i.e., SGD. The syntax is:\n",
    "**sklearn.linear_model.SGDRegressor()**\n",
    "- eta0: the initial learning rate\n",
    "\n",
    "The third variation is most frequently used in deep learning and can be applied in keras. It takes some effort to install keras, and we will save this part to the deep learning sections.\n",
    "\n",
    "Batch gradient descent is rarely seen in python packages. Here I will provide a template to code up this variation manually.\n",
    "\n",
    "Now let's apply (the first two) gradient descent algorithms to the linear model to see if we obtain the same result from the LinearRegression() function.\n",
    "\n",
    "\n",
    "**Practice**\n",
    "- Obtain the coefficient for the intercept using LinearRegression().\n",
    "- Obtain the coefficient for the intercept using SGDRegressor(). Set initial learning rate to 0.01\n",
    "- Obtain the coefficient for the intercept by doing a batch gradient descent manually. Set initial learning rate to 0.01. Assume convergence after 1000 iterations (in practice, need to check the change in cost function).\n",
    "- Check if the reported coefficients are close to 3 (i.e., the true value)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.0338873093107876\n"
     ]
    }
   ],
   "source": [
    "# Linear Regression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "model = LinearRegression()\n",
    "model.fit(X,y)\n",
    "print(model.intercept_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3.03799072]\n"
     ]
    }
   ],
   "source": [
    "# SGD Regressor\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "\n",
    "sgd_model = SGDRegressor(eta0 = 0.01)\n",
    "\n",
    "sgd_model.fit(X, y)\n",
    "\n",
    "print(sgd_model.intercept_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.0407622760646515\n"
     ]
    }
   ],
   "source": [
    "# Now, let's do a naive batch gradient descent manually\n",
    "\n",
    "# S1. include a column of 1s to the original X matrix. \n",
    "##The estimation of the new variable (i.e., column of 1) is the intercept.\n",
    "\n",
    "b = np.append(X,np.ones([len(X),1]),1)\n",
    "# b is our new X matrix, the last column b[500] estimates the intercept\n",
    "\n",
    "\n",
    "# S2. Initialization, set iteration number, learning rate, and a set of initial thetas.\n",
    "n_iterations = 1000\n",
    "eta = 0.01\n",
    "\n",
    "theta = np.random.randn(b.shape[1]) # set initial thetas, they are random draws from standard normal distribution\n",
    "m = X.shape[0]\n",
    "\n",
    "# S3. Use a for-loop to accommodate the iterations\n",
    "\n",
    "for iteration in range(n_iterations):\n",
    "    gradients = 2/m * b.T.dot(b.dot(theta) - y)\n",
    "    theta = theta - eta * gradients\n",
    "\n",
    "# b'(b * theta - y)\n",
    "\n",
    "# S4. Print results    \n",
    "print(theta[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
